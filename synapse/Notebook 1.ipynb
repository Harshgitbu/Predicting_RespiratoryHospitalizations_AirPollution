{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 128,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "# Set up the configuration for accessing the storage account\n",
        "storage_account_name = \"team4storage\"\n",
        "storage_account_key = \"cqzd7Fys8ii2ocJXH4eZA1P+1LKHbgvzOYtx55kqDmsL3fck8ggndqsuch8Mdh0YUDHBhqr6uyQD+ASttatJWA==\"\n",
        "container = \"team4container\"\n",
        "\n",
        "\n",
        "\n",
        "spark.conf.set(\n",
        "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
        "    storage_account_key\n",
        ")\n",
        "\n",
        "wasbs_path = f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"\n",
        "\n",
        "\n",
        "# Load the weather data from the silver layer\n",
        "respiratory_Boston_df = spark.read.parquet(f\"{wasbs_path}Silver/RespiratoryHospitalizations/RespiratoryHospitalizationsFilteredBoston.parquet\")\n",
        "respiratory_Houston_df = spark.read.parquet(f\"{wasbs_path}Silver/RespiratoryHospitalizations/RespiratoryHospitalizationsFilteredHouston.parquet\")\n",
        "\n",
        "# Load the air pollution data\n",
        "air_pollution_Boston_df =  spark.read.parquet(f\"{wasbs_path}Silver/HistoricalAirPollution/HistoricalAirPollutionBoston.parquet\")\n",
        "air_pollution_Houston_df =  spark.read.parquet(f\"{wasbs_path}Silver/HistoricalAirPollution/HistoricalAirPollutionHouston.parquet\")\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import DoubleType, FloatType\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Helper function to cast the columns to FloatType\n",
        "def cast_air_pollution_df(df):\n",
        "    # Cast the columns to FloatType\n",
        "    df = (\n",
        "        df.withColumn(\"co\", col(\"co\").cast(DoubleType()))\n",
        "          .withColumn(\"nh3\", col(\"nh3\").cast(DoubleType()))\n",
        "          .withColumn(\"no\", col(\"no\").cast(IntegerType()))\n",
        "          .withColumn(\"no2\", col(\"no2\").cast(DoubleType()))\n",
        "          .withColumn(\"o3\", col(\"o3\").cast(DoubleType()))\n",
        "          .withColumn(\"pm10\", col(\"pm10\").cast(DoubleType()))\n",
        "          .withColumn(\"pm2_5\", col(\"pm2_5\").cast(DoubleType()))\n",
        "          .withColumn(\"so2\", col(\"so2\").cast(DoubleType()))\n",
        "    )\n",
        "    \n",
        "\n",
        "    return df\n",
        "\n",
        "# Cast and check\n",
        "air_pollution_Boston_df = cast_air_pollution_df(air_pollution_Boston_df)\n",
        "air_pollution_Houston_df = cast_air_pollution_df(air_pollution_Houston_df)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "\n",
        "def calculate_aqi(pollutant, concentration):\n",
        "    breakpoints = {\n",
        "        'O3_8hr': [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150), (86, 105, 151, 200), (106, 200, 201, 300)],\n",
        "        'O3_1hr': [(125, 164, 101, 150), (165, 204, 151, 200), (205, 404, 201, 300), (405, 504, 301, 400), (505, 604, 401, 500)],\n",
        "        'PM2.5_24hr': [(0, 12, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150), (55.5, 150.4, 151, 200), (150.5, 250.4, 201, 300), (250.5, 350.4, 301, 400), (350.5, 500.4, 401, 500)],\n",
        "        'PM10_24hr': [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150), (255, 354, 151, 200), (355, 424, 201, 300), (425, 504, 301, 400), (505, 604, 401, 500)],\n",
        "        'CO_8hr': [(0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150), (12.5, 15.4, 151, 200), (15.5, 30.4, 201, 300), (30.5, 40.4, 301, 400), (40.5, 50.4, 401, 500)],\n",
        "        'SO2_1hr': [(0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150), (186, 304, 151, 200)],\n",
        "        'SO2_24hr': [(305, 604, 201, 300), (605, 804, 301, 400), (805, 1004, 401, 500)],\n",
        "        'NO2_1hr': [(0, 53, 0, 50), (54, 100, 51, 100), (101, 360, 101, 150), (361, 649, 151, 200), (650, 1249, 201, 300), (1250, 2049, 301, 400), (2050, 3049, 401, 500)],\n",
        "    }\n",
        "\n",
        "    if pollutant not in breakpoints:\n",
        "        raise ValueError(f\"Unsupported pollutant: {pollutant}\")\n",
        "\n",
        "    for (Clow, Chigh, Ilow, Ihigh) in breakpoints[pollutant]:\n",
        "        if Clow <= concentration <= Chigh:\n",
        "            # Use F.round() instead of Python's round() to handle columns\n",
        "            return round(((Ihigh - Ilow) / (Chigh - Clow)) * (concentration - Clow) + Ilow)\n",
        "\n",
        "    return None  # If the concentration is out of the given ranges\n",
        "\n",
        "def calculate_rolling_average(df, column, window):\n",
        "    windowSpec = Window.partitionBy(\"Location\").orderBy(\"date_time\").rowsBetween(-window+1, 0)\n",
        "    return F.avg(column).over(windowSpec)\n",
        "\n",
        "# Modify calculate_aqi_row to work with PySpark Columns\n",
        "def calculate_aqi_row(row):\n",
        "    pollutant_map = {\n",
        "        'o3_8hr': 'O3_8hr',\n",
        "        'o3_1hr': 'O3_1hr',\n",
        "        'pm2_5_24hr': 'PM2.5_24hr',\n",
        "        'pm10_24hr': 'PM10_24hr',\n",
        "        'co_8hr': 'CO_8hr',\n",
        "        'so2_1hr': 'SO2_1hr',\n",
        "        'so2_24hr': 'SO2_24hr',\n",
        "        'no2_1hr': 'NO2_1hr'\n",
        "    }\n",
        "    aqi_values = []\n",
        "\n",
        "    for col_name, pollutant in pollutant_map.items():\n",
        "        concentration = row[col_name]\n",
        "        if concentration is not None:\n",
        "            aqi = calculate_aqi(pollutant, concentration)\n",
        "            if aqi is not None:\n",
        "                aqi_values.append(aqi)\n",
        "\n",
        "    if aqi_values:\n",
        "        return max(aqi_values)\n",
        "    return None\n",
        "\n",
        "# Apply the AQI calculation to your DataFrame\n",
        "def calculate_us_aqi(df):\n",
        "    df = df.withColumn(\"o3_8hr\", calculate_rolling_average(df, \"o3\", 8))\n",
        "    df = df.withColumn(\"o3_1hr\", df[\"o3\"])\n",
        "    df = df.withColumn(\"pm2_5_24hr\", calculate_rolling_average(df, \"pm2_5\", 24))\n",
        "    df = df.withColumn(\"pm10_24hr\", calculate_rolling_average(df, \"pm10\", 24))\n",
        "    df = df.withColumn(\"co_8hr\", calculate_rolling_average(df, \"co\", 8))\n",
        "    df = df.withColumn(\"so2_1hr\", df[\"so2\"])\n",
        "    df = df.withColumn(\"so2_24hr\", calculate_rolling_average(df, \"so2\", 24))\n",
        "    df = df.withColumn(\"no2_1hr\", df[\"no2\"])\n",
        "\n",
        "    calculate_aqi_udf = F.udf(lambda row: calculate_aqi_row(row), IntegerType())\n",
        "    \n",
        "    df = df.withColumn(\"us_aqi\", calculate_aqi_udf(F.struct(\n",
        "        col(\"o3_8hr\"),\n",
        "        col(\"o3_1hr\"),\n",
        "        col(\"pm2_5_24hr\"),\n",
        "        col(\"pm10_24hr\"),\n",
        "        col(\"co_8hr\"),\n",
        "        col(\"so2_1hr\"),\n",
        "        col(\"so2_24hr\"),\n",
        "        col(\"no2_1hr\")\n",
        "    )))\n",
        "\n",
        "    return df\n",
        "# air_pollution_Boston_df.head(10)\n",
        "\n",
        "# # Now re-run the AQI calculation\n",
        "air_pollution_Boston_df = calculate_us_aqi(air_pollution_Boston_df)\n",
        "air_pollution_Houston_df = calculate_us_aqi(air_pollution_Houston_df)\n",
        "\n",
        "\n",
        "\n",
        "# Daily Average AQI: Compute the daily average Air Quality Index (AQI)\n",
        "aqi_agg_Boston_df = air_pollution_Boston_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
        "    F.round(F.avg(\"us_aqi\")).alias(\"avg_us_aqi\")\n",
        ")\n",
        "\n",
        "aqi_agg_Houston_df = air_pollution_Houston_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
        "    F.round(F.avg(\"us_aqi\")).alias(\"avg_us_aqi\")\n",
        ")\n",
        "\n",
        "# Pollutant Aggregation: Calculate daily averages for each pollutant\n",
        "pollutant_agg_Boston_df = air_pollution_Boston_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
        "    F.round(F.avg(\"co\"),2).alias(\"avg_co\"),\n",
        "    F.round(F.avg(\"no2\"),2).alias(\"avg_no2\"),\n",
        "    F.round(F.avg(\"o3\"),2).alias(\"avg_o3\"),\n",
        "    F.round(F.avg(\"so2\"),2).alias(\"avg_so2\"),\n",
        "    F.round(F.avg(\"pm2_5\"),2).alias(\"avg_pm2_5\"),\n",
        "    F.round(F.avg(\"pm10\"),2).alias(\"avg_pm10\")\n",
        ")\n",
        "\n",
        "pollutant_agg_Houston_df = air_pollution_Houston_df.groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
        "    F.round(F.avg(\"co\"),2).alias(\"avg_co\"),\n",
        "    F.round(F.avg(\"no2\"),2).alias(\"avg_no2\"),\n",
        "    F.round(F.avg(\"o3\"),2).alias(\"avg_o3\"),\n",
        "    F.round(F.avg(\"so2\"),2).alias(\"avg_so2\"),\n",
        "    F.round(F.avg(\"pm2_5\"),2).alias(\"avg_pm2_5\"),\n",
        "    F.round(F.avg(\"pm10\"),2).alias(\"avg_pm10\")\n",
        ")\n",
        "\n",
        "\n",
        "# Identify High Pollution Events: Mark days with high pollution levels based on a specified threshold\n",
        "high_pollution_events_Boston_df = air_pollution_Boston_df.withColumn(\"high_pollution\", F.when(col(\"us_aqi\") > 100, 1).otherwise(0)).groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
        "    F.sum(\"high_pollution\").alias(\"high_pollution_events\")\n",
        ")\n",
        "# Identify High Pollution Events: Mark days with high pollution levels based on a specified threshold\n",
        "high_pollution_events_Houston_df = air_pollution_Houston_df.withColumn(\"high_pollution\", F.when(col(\"us_aqi\") > 100, 1).otherwise(0)).groupBy(F.date_format(col(\"date_time\"), \"yyyy-MM-dd\").alias(\"date\")).agg(\n",
        "    F.sum(\"high_pollution\").alias(\"high_pollution_events\")\n",
        ")\n",
        "\n",
        "\n",
        "# Save aggregated data as single files\n",
        "# can modify this to go directly to your directories\n",
        "# aqi_agg_Boston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/agg_Boston_aqi\")\n",
        "# aqi_agg_Houston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/agg_Houston_aqi\")\n",
        "# pollutant_agg_Boston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/agg_Boston_pollutants\")\n",
        "# pollutant_agg_Houston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/agg_Houston_pollutants\")\n",
        "# high_pollution_events_Boston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/agg_Boston_high_pollution_events\")\n",
        "# high_pollution_events_Houston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/agg_Houston_high_pollution_events\")\n",
        "\n",
        "# Save processed air pollution data\n",
        "# air_pollution_Boston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/Boston_processed_air_pollution\")\n",
        "# air_pollution_Houston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/Houston_processed_air_pollution\")\n",
        "\n",
        "def cast_respiratory_df(df):\n",
        "    # Cast the columns to IntegerType where appropriate\n",
        "    df = (\n",
        "        df.withColumn(\"total_visits_covid\", col(\"total_visits_covid\").cast(IntegerType()))\n",
        "          .withColumn(\"total_visits_influenza\", col(\"total_visits_influenza\").cast(IntegerType()))\n",
        "          .withColumn(\"total_visits_rsv\", col(\"total_visits_rsv\").cast(IntegerType()))\n",
        "          .withColumn(\"total_visits_smoothed_combined\", col(\"total_visits_smoothed_combined\").cast(DoubleType()))\n",
        "          .withColumn(\"total_visits_smoothed_covid\", col(\"total_visits_smoothed_covid\").cast(DoubleType()))\n",
        "          .withColumn(\"total_visits_smoothed_influenza\", col(\"total_visits_smoothed_influenza\").cast(DoubleType()))\n",
        "          .withColumn(\"total_visits_smoothed_rsv\", col(\"total_visits_smoothed_rsv\").cast(DoubleType()))\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Cast columns\n",
        "respiratory_Boston_df = cast_respiratory_df(respiratory_Boston_df)\n",
        "respiratory_Houston_df = cast_respiratory_df(respiratory_Houston_df)\n",
        "\n",
        "def identify_high_visit_events_Bos(df, threshold_covid = 27125, threshold_flu = 441755, threshold_rsv = 10462):\n",
        "    # Mark high visit events based on threshold\n",
        "    df = df.withColumn(\"high_visits_covid\", F.when(col(\"total_visits_covid\") > threshold_covid, 1).otherwise(0))\n",
        "    df = df.withColumn(\"high_visits_influenza\", F.when(col(\"total_visits_influenza\") > threshold_flu, 1).otherwise(0))\n",
        "    df = df.withColumn(\"high_visits_rsv\", F.when(col(\"total_visits_rsv\") > threshold_rsv, 1).otherwise(0))\n",
        "    return df\n",
        "\n",
        "def identify_high_visit_events_Hou(df, threshold_covid = 175325, threshold_flu = 285530, threshold_rsv = 67625):\n",
        "    # Mark high visit events based on threshold\n",
        "    df = df.withColumn(\"high_visits_covid\", F.when(col(\"total_visits_covid\") > threshold_covid, 1).otherwise(0))\n",
        "    df = df.withColumn(\"high_visits_influenza\", F.when(col(\"total_visits_influenza\") > threshold_flu, 1).otherwise(0))\n",
        "    df = df.withColumn(\"high_visits_rsv\", F.when(col(\"total_visits_rsv\") > threshold_rsv, 1).otherwise(0))\n",
        "    return df\n",
        "# Identify high visit events for Boston and Houston\n",
        "high_visit_events_Boston_df = identify_high_visit_events_Bos(respiratory_Boston_df)\n",
        "high_visit_events_Houston_df = identify_high_visit_events_Hou(respiratory_Houston_df)\n",
        "\n",
        "\n",
        "# high_visit_events_Boston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/agg_Boston_high_visit_events\")\n",
        "# high_visit_events_Houston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/agg_Houston_high_visit_events\")\n",
        "# respiratory_Boston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/Boston_processed_respiratory\")\n",
        "# respiratory_Houston_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"{wasbs_path}Gold/Houston_processed_respiratory\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}